# âœ… Spec Evaluation Criteria by Stakeholder Persona

Each stakeholder evaluates a spec based on their unique goals and concerns. Use these criteria to assess and refine specs before work begins.

---

## ğŸ§­ Product Manager (PM)

1. **Clarity of Problem Statement**
   - **1** â€“ Vague, jargon-filled, or missing entirely.
   - **3** â€“ Reasonably clear, but lacks context or business framing.
   - **5** â€“ Clearly states the problem, affected users, and why it matters.
   - **Success:** Any stakeholder can understand the â€œwhyâ€ within 30 seconds.

2. **User & Business Impact**
   - **1** â€“ No measurable impact or user value identified.
   - **3** â€“ High-level impact noted but lacks detail or evidence.
   - **5** â€“ Specific, measurable outcomes tied to product or business goals.
   - **Success:** Spec includes hypotheses, OKRs, or KPIs to track.

3. **Prioritization Justification**
   - **1** â€“ No rationale for why this is being prioritized now.
   - **3** â€“ Mentions timing or urgency but lacks comparison to alternatives.
   - **5** â€“ Explicit reasoning about trade-offs, urgency, and opportunity cost.
   - **Success:** PM can defend this priority choice in roadmap reviews.

4. **Scope Definition**
   - **1** â€“ Scope creep risk: unclear boundaries or adjacent features mentioned.
   - **3** â€“ Rough scoping with known unknowns but lacks clarity on exclusions.
   - **5** â€“ Clear in-scope, out-of-scope, and future work demarcation.
   - **Success:** Engineering can estimate confidently with minimal follow-ups.

5. **Alignment with Strategy**
   - **1** â€“ No connection to company, team, or user strategy.
   - **3** â€“ Loosely references strategic pillars or themes.
   - **5** â€“ Explicitly maps to strategic goals with links or references.
   - **Success:** PM can point to a strategic document that justifies the spec.

---

## âš™ï¸ Engineering Lead / Staff Engineer

1. **Technical Feasibility**
   - **1** â€“ Mentions features without considering existing systems.
   - **3** â€“ Assumes feasibility but lacks validation or constraints.
   - **5** â€“ Includes engineering feedback, feasibility notes, or spike results.
   - **Success:** No major re-scoping needed during implementation.

2. **Defined Interfaces and Dependencies**
   - **1** â€“ No mention of systems, APIs, or dependencies.
   - **3** â€“ High-level references to integration points, but unclear ownership.
   - **5** â€“ Precise definitions of dependencies, contracts, and teams involved.
   - **Success:** Engineers know exactly what needs to be built and where it plugs in.

3. **Risk Assessment**
   - **1** â€“ No risks listed or hand-waved as low.
   - **3** â€“ Generic risks listed with no mitigations.
   - **5** â€“ Realistic, project-specific risks with mitigation plans.
   - **Success:** Known risks are proactively flagged and de-risked.

4. **Edge Cases & Failure Modes**
   - **1** â€“ No mention of edge cases or system failures.
   - **3** â€“ Mentions some cases but lacks completeness.
   - **5** â€“ Clearly thought through â€œwhat ifs,â€ failure handling, and fallback behavior.
   - **Success:** Reviewers canâ€™t easily surface missing cases.

5. **Estimability**
   - **1** â€“ Ambiguous requirements prevent effort estimates.
   - **3** â€“ Partial clarity; engineers need follow-up to size tasks.
   - **5** â€“ Well-scoped with story-level clarity for ticketing.
   - **Success:** Tasks can be broken down and assigned with confidence.

---

## ğŸ¨ Design Lead / UX Researcher

1. **User Journey Coverage**
   - **1** â€“ No mention of user flow or interaction context.
   - **3** â€“ Touches on UX but missing key flows or entry points.
   - **5** â€“ End-to-end user flows and edge scenarios described.
   - **Success:** Designers know what screens or components are needed.

2. **Design System Integration**
   - **1** â€“ Introduces new UI elements without justification.
   - **3** â€“ Mentions design system but lacks references or specs.
   - **5** â€“ Uses existing components or justifies new additions.
   - **Success:** Designs can reuse components with minimal custom styling.

3. **Accessibility Considerations**
   - **1** â€“ Ignores accessibility altogether.
   - **3** â€“ Mentions it, but with no specific concerns addressed.
   - **5** â€“ Identifies potential accessibility challenges and guidelines.
   - **Success:** Designers can confidently apply WCAG or internal standards.

4. **User Validation Evidence**
   - **1** â€“ No user research or signal cited.
   - **3** â€“ Anecdotal feedback or outdated studies referenced.
   - **5** â€“ Synthesizes recent user interviews, surveys, or behavioral data.
   - **Success:** Decisions are grounded in real user pain.

5. **Design/Engineering Handoff Readiness**
   - **1** â€“ Ambiguous descriptions; leaves major UX gaps.
   - **3** â€“ Provides general design intent.
   - **5** â€“ Clear specs, links to wireframes/prototypes, responsive behavior defined.
   - **Success:** Engineers can build without needing to guess interactions.

---

## ğŸ“Š Data / Analytics Partner

1. **Success Metrics Defined**
   - **1** â€“ No metrics or unclear definitions.
   - **3** â€“ Mentions metrics without formulas or context.
   - **5** â€“ Defines primary/secondary metrics, formulas, and thresholds.
   - **Success:** Everyone can tell if the project is successful post-launch.

2. **Instrumentation Plan**
   - **1** â€“ No mention of events or data tracking.
   - **3** â€“ Mentions logging but lacks detail.
   - **5** â€“ Specific events, properties, and tagging outlined.
   - **Success:** Instrumentation tickets can be written directly from the spec.

3. **Experimentation Strategy**
   - **1** â€“ No A/B plan or control group identified.
   - **3** â€“ Generic mention of an experiment but lacks design.
   - **5** â€“ Clear test/control groups, sample size needs, and hypotheses.
   - **Success:** The team can launch and interpret an A/B test with confidence.

4. **Data Dependencies**
   - **1** â€“ No mention of upstream or downstream data impacts.
   - **3** â€“ Lists some dependencies but unclear source or freshness.
   - **5** â€“ Describes exact data sources, latency, ownership.
   - **Success:** Data flows are known and reliable for analysis.

5. **Dashboard / Reporting Plan**
   - **1** â€“ No dashboard or plan to review performance.
   - **3** â€“ Mentions reporting but unclear metrics or audience.
   - **5** â€“ Lists metrics, audience, cadence, and owner.
   - **Success:** Team knows where and how performance will be monitored.

---

## ğŸ§‘â€ğŸ’¼ Cross-Functional Stakeholder (e.g., Legal, Marketing, Executive)

1. **Compliance & Regulatory Checks**
   - **1** â€“ No mention of legal/privacy implications.
   - **3** â€“ Mentions "check with legal" but no specifics.
   - **5** â€“ Flags data usage, consent flows, TOS implications.
   - **Success:** Legal partner can review and sign off in <1 day.

2. **Brand Voice & Positioning**
   - **1** â€“ Conflicts with brand guidelines or market messaging.
   - **3** â€“ Brand-conscious but lacks review from comms or marketing.
   - **5** â€“ Aligned with tone, value prop, and visual identity.
   - **Success:** Stakeholders feel confident putting it in front of customers.

3. **Go-To-Market Dependencies**
   - **1** â€“ No mention of comms, support, or training needs.
   - **3** â€“ Mentions launch timing but no downstream coordination.
   - **5** â€“ Lists GTM tasks, owners, and timing implications.
   - **Success:** GTM teams know when and how to support launch.

4. **Cost & Resource Awareness**
   - **1** â€“ Assumes free or unbounded resources.
   - **3** â€“ Mentions cost but no estimate or trade-offs.
   - **5** â€“ Flags infra, headcount, or tooling needs with impact.
   - **Success:** Leadership can assess ROI or trade-offs.

5. **Stakeholder Visibility**
   - **1** â€“ No stakeholders or reviewers mentioned.
   - **3** â€“ Vague list of "to check with" roles.
   - **5** â€“ Named stakeholders with clear sign-off expectations.
   - **Success:** Everyone knows who owns approval and feedback.

---
